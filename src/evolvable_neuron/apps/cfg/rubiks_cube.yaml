seed:
    train: 934
    evaluation: 990

logger:
    _target_: jumanji.training.loggers.TensorboardLogger
    name: learn-jumanji
    save_checkpoint: False

environment_id: RubiksCube-partly-scrambled-v0

evaluators:
    # evaluation of the action-selection scheme used during training
    stochastic:
        _target_: evolvable_neuron.agent.Evaluator
        _partial_: True
        total_batch_size: 1024
        stochastic: True
    # evaluation of an argmax action-selection scheme
    greedy:
        _target_: evolvable_neuron.agent.Evaluator
        _partial_: True
        total_batch_size: 1024
        stochastic: False

training:
    # Number of environments that run in parallel. This is also the batch size
    envs_in_parallel: 256
    # Number of single environment steps after a SGD update (aka. iteration)
    env_steps_per_update: 10
    # Number of iterations after each evaluation session (aka. epoch)
    num_learner_updates_per_epoch: 100
    # Duration of training (sec)
    time_budget: 300

env:
    # 'RubiksCube-v0' has a time limit of 200, 'RubiksCube-partly-scrambled-v0' of 20
    time_limit: 20
    # For 'RubiksCube-v0': ``env.unwrapped.action_spec().num_values == [6, 1, 3]``, hence
    # the number of actions is 6 * 1 * 3 == 18
    action_spec_num_values: '[6, 1, 3]'
    num_actions: 18

agent:
    _target_: evolvable_neuron.agent.a2c.Agent
    _partial_: True
    n_steps: ${training.env_steps_per_update}
    total_batch_size: ${training.envs_in_parallel}
    policy:
        _target_: evolvable_neuron.networks.rubiks_cube.actor
        cube_embed_dim: 4
        time_limit: ${env.time_limit}
        step_count_embed_dim: 4
        dense_layer_dims: ${oc.decode:'[256, 256]'}
        num_actions: ${env.num_actions}
    value:
        _target_: evolvable_neuron.networks.rubiks_cube.critic
        cube_embed_dim: 4
        dense_layer_dims: ${oc.decode:'[256, 256]'}
        time_limit: ${env.time_limit}
        step_count_embed_dim: 4
    parametric_action_distribution:
        _target_: jumanji.training.networks.parametric_distribution.FactorisedActionSpaceParametricDistribution
        action_spec_num_values:
            _target_: numpy.array
            object: ${oc.decode:${env.action_spec_num_values}}
            dtype: int32

    optimizer:
        _target_: optax.adam
        learning_rate: 1e-3
    normalize_advantage: False
    discount_factor: ???
    bootstrapping_factor: ???
    l_pg: ???
    l_td: ???
    l_en: ???
